{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteoc/miniconda3/envs/speech-meg/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import scipy.spatial as sp\n",
    "\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 37.19it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape true images tensor: torch.Size([100, 3, 480, 480])\n",
      "Shape reconstructed images tensor: torch.Size([100, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "save_true_dir = \"/srv/nfs-data/sisko/matteoc/monkeys/generative/img_real\"\n",
    "save_recon_dir = \"/srv/nfs-data/sisko/matteoc/monkeys/generative/img_gen_linavg\"\n",
    "true_image_files = sorted(os.listdir(save_true_dir))\n",
    "recon_image_files = sorted(os.listdir(save_recon_dir))\n",
    "\n",
    "true_images = []\n",
    "recon_images = []\n",
    "easy_transform = transforms.ToTensor()\n",
    "transform_real = transforms.Compose([\n",
    "    transforms.Resize((480, 480)),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "for filename in tqdm(true_image_files):\n",
    "    img_path = os.path.join(save_true_dir, filename)\n",
    "    img = Image.open(img_path).convert(\"RGB\")  \n",
    "    img_tensor = transform_real(img)\n",
    "    true_images.append(img_tensor)\n",
    "\n",
    "for filename in tqdm(recon_image_files):\n",
    "    img_path = os.path.join(save_recon_dir, filename)\n",
    "    img = Image.open(img_path).convert(\"RGB\") \n",
    "    img_tensor = easy_transform(img)\n",
    "    recon_images.append(img_tensor)\n",
    "\n",
    "true_images_tensor = torch.stack(true_images) \n",
    "recon_images_tensor = torch.stack(recon_images) \n",
    "\n",
    "print(\"Shape true images tensor:\", true_images_tensor.shape)\n",
    "print(\"Shape reconstructed images tensor:\", recon_images_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def two_way_identification(all_brain_recons, all_images, model, preprocess, feature_layer=None, return_avg=True, device='cuda:3'):\n",
    "    preds = model(torch.stack([preprocess(recon) for recon in all_brain_recons], dim=0).to(device))\n",
    "    reals = model(torch.stack([preprocess(indiv) for indiv in all_images], dim=0).to(device))\n",
    "    if feature_layer is None:\n",
    "        preds = preds.float().flatten(1).cpu().numpy()\n",
    "        reals = reals.float().flatten(1).cpu().numpy()\n",
    "    else:\n",
    "        preds = preds[feature_layer].float().flatten(1).cpu().numpy()\n",
    "        reals = reals[feature_layer].float().flatten(1).cpu().numpy()\n",
    "\n",
    "    r = np.corrcoef(reals, preds)\n",
    "    r = r[:len(all_images), len(all_images):]\n",
    "    congruents = np.diag(r)\n",
    "\n",
    "    success = r < congruents\n",
    "    success_cnt = np.sum(success, 0)\n",
    "\n",
    "    if return_avg:\n",
    "        perf = np.mean(success_cnt) / (len(all_images)-1)\n",
    "        return perf\n",
    "    else:\n",
    "        return success_cnt, len(all_images)-1\n",
    "    \n",
    "\n",
    "def encode_image_hf(clip_model, img_batch):\n",
    "    return clip_model.get_image_features(pixel_values=img_batch)\n",
    "\n",
    "\n",
    "def cal_metrics(all_images, all_brain_recons, device):\n",
    "    all_images = all_images[:].to(device)\n",
    "    all_brain_recons = torch.stack([img for img in all_brain_recons[:]]).to(device).to(all_images.dtype).clamp(0,1).squeeze()\n",
    "\n",
    "    print(\"Images shape:\", all_images.shape)\n",
    "    print(\"Recons shape:\", all_brain_recons.shape)\n",
    "\n",
    "    # Ensure both tensors are the same size for MSE\n",
    "    resize = transforms.Resize((all_images.size(2), all_images.size(3)), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "    all_brain_recons = resize(all_brain_recons)\n",
    "\n",
    "    print(\"Images shape after resize:\", all_images.shape)\n",
    "    print(\"Recons shape after resize:\", all_brain_recons.shape)\n",
    "\n",
    "    # Preprocess\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    ])\n",
    "\n",
    "    # Flatten images while keeping the batch dimension\n",
    "    all_images_flattened = preprocess(all_images).reshape(len(all_images), -1).to(device).cpu()\n",
    "    all_brain_recons_flattened = preprocess(all_brain_recons).view(len(all_brain_recons), -1).cpu()\n",
    "\n",
    "    print(all_images_flattened.shape)\n",
    "    print(all_brain_recons_flattened.shape)\n",
    "\n",
    "    # PixCorr\n",
    "    print(\"\\n------calculating pixcorr------\")\n",
    "    corrsum = 0\n",
    "    for i in tqdm(range(len(all_images))):\n",
    "        corrsum += np.corrcoef(all_images_flattened[i], all_brain_recons_flattened[i])[0][1]\n",
    "    pixcorr = corrsum / len(all_images)\n",
    "    print(\"PixCorr:\", pixcorr)\n",
    "\n",
    "    # SSIM\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(625, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    ])\n",
    "\n",
    "    img_gray = rgb2gray(preprocess(all_images).permute((0,2,3,1)).cpu().numpy())\n",
    "    recon_gray = rgb2gray(preprocess(all_brain_recons).permute((0,2,3,1)).cpu().numpy())\n",
    "    print(\"converted, now calculating ssim...\")\n",
    "\n",
    "    ssim_score=[]\n",
    "    for im, rec in tqdm(zip(img_gray, recon_gray), total=len(all_images)):\n",
    "        ssim_score.append(ssim(rec, im, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0))\n",
    "\n",
    "    ssim_mean = np.mean(ssim_score)\n",
    "    print(\"SSIM:\", ssim_mean)\n",
    "\n",
    "    # MSE\n",
    "    mse = torch.nn.functional.mse_loss(all_brain_recons, all_images).item()\n",
    "    print(\"MSE:\", mse)\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(all_brain_recons_flattened, all_images_flattened).mean().item()\n",
    "    print(\"Cosine Similarity:\", cosine_sim)\n",
    "\n",
    "    # Feature-based evaluations using different models\n",
    "    def evaluate_model(model, preprocess, feature_layers, layer_names):\n",
    "        results = {}\n",
    "        for feature_layer, layer_name in zip(feature_layers, layer_names):\n",
    "            print(f\"\\n---{layer_name}---\")\n",
    "            all_per_correct = two_way_identification(all_brain_recons.to(device).float(), all_images, \n",
    "                                                     model, preprocess, feature_layer, device=device)\n",
    "            results[layer_name] = np.mean(all_per_correct)\n",
    "            print(f\"2-way Percent Correct: {results[layer_name]:.4f}\")\n",
    "        return results\n",
    "\n",
    "    # AlexNet\n",
    "    from torchvision.models import alexnet, AlexNet_Weights\n",
    "    alex_weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "    alex_model = create_feature_extractor(alexnet(weights=alex_weights), return_nodes=['features.4', 'features.11']).to(device)\n",
    "    alex_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    alexnet_results = evaluate_model(alex_model, preprocess, ['features.4', 'features.11'], ['AlexNet(2)', 'AlexNet(5)'])\n",
    "    del alex_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # InceptionV3\n",
    "    from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "    inception_weights = Inception_V3_Weights.DEFAULT\n",
    "    inception_model = create_feature_extractor(inception_v3(weights=inception_weights), return_nodes=['avgpool']).to(device)\n",
    "    inception_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    inception_results = evaluate_model(inception_model, preprocess, ['avgpool'], ['InceptionV3'])\n",
    "    del inception_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # CLIP\n",
    "    clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "    # clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                            std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    ])\n",
    "\n",
    "    all_per_correct = two_way_identification(all_brain_recons, all_images,\n",
    "                                            clip_model.encode_image, preprocess, None) # final layer\n",
    "    clip_results = np.mean(all_per_correct)\n",
    "    print(\"CLIP:\", clip_results)\n",
    "\n",
    "    # EfficientNet\n",
    "    from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "    eff_weights = EfficientNet_B1_Weights.DEFAULT\n",
    "    eff_model = create_feature_extractor(efficientnet_b1(weights=eff_weights), return_nodes=['avgpool']).to(device)\n",
    "    eff_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(255, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    gt = eff_model(preprocess(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt), -1).cpu().numpy()\n",
    "    fake = eff_model(preprocess(all_brain_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake), -1).cpu().numpy()\n",
    "    effnet_distance = np.array([sp.distance.correlation(gt[i], fake[i]) for i in range(len(gt))]).mean()\n",
    "    print(\"EffNet Distance:\", effnet_distance)\n",
    "    del eff_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # SwAV\n",
    "    swav_model = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
    "    swav_model = create_feature_extractor(swav_model, return_nodes=['avgpool']).to(device)\n",
    "    swav_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    gt = swav_model(preprocess(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt), -1).cpu().numpy()\n",
    "    fake = swav_model(preprocess(all_brain_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake), -1).cpu().numpy()\n",
    "    swav_distance = np.array([correlation(gt[i], fake[i]) for i in range(len(gt))]).mean()\n",
    "    print(\"SwAV Distance:\", swav_distance)\n",
    "    del swav_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Save the results\n",
    "    metrics = {\n",
    "        'PixCorr': [pixcorr],\n",
    "        'SSIM': [ssim_mean],\n",
    "        'MSE': [mse],\n",
    "        'Cosine Similarity': [cosine_sim],\n",
    "        'AlexNet(2)': [alexnet_results[\"AlexNet(2)\"]],\n",
    "        'AlexNet(5)': [alexnet_results[\"AlexNet(5)\"]],\n",
    "        'InceptionV3': [inception_results[\"InceptionV3\"]],\n",
    "        'CLIP': [clip_results],  # corrected line\n",
    "        'EffNet Distance': [effnet_distance],\n",
    "        'SwAV Distance': [swav_distance]\n",
    "    }\n",
    "    return metrics \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def two_way_identification(all_brain_recons, all_images, model, preprocess, feature_layer=None, return_avg=True, device='cuda:3'):\n",
    "    preds = model(torch.stack([preprocess(recon) for recon in all_brain_recons], dim=0).to(device))\n",
    "    reals = model(torch.stack([preprocess(indiv) for indiv in all_images], dim=0).to(device))\n",
    "    if feature_layer is None:\n",
    "        preds = preds.float().flatten(1).cpu().numpy()\n",
    "        reals = reals.float().flatten(1).cpu().numpy()\n",
    "    else:\n",
    "        preds = preds[feature_layer].float().flatten(1).cpu().numpy()\n",
    "        reals = reals[feature_layer].float().flatten(1).cpu().numpy()\n",
    "\n",
    "    r = np.corrcoef(reals, preds)\n",
    "    r = r[:len(all_images), len(all_images):]\n",
    "    congruents = np.diag(r)\n",
    "\n",
    "    success = r < congruents\n",
    "    success_cnt = np.sum(success, 0)\n",
    "\n",
    "    # if return_avg:\n",
    "    #     perf = np.mean(success_cnt) / (len(all_images)-1)\n",
    "    #     return perf\n",
    "    # else:\n",
    "    #     return success_cnt, len(all_images)-1\n",
    "\n",
    "    if return_avg:\n",
    "        perf_per_sample = success_cnt / (len(all_images)-1)\n",
    "        return perf_per_sample  # restituisce tutti i valori\n",
    "    else:\n",
    "        return success_cnt, len(all_images)-1\n",
    "\n",
    "    \n",
    "\n",
    "def encode_image_hf(clip_model, img_batch):\n",
    "    return clip_model.get_image_features(pixel_values=img_batch)\n",
    "\n",
    "\n",
    "def cal_metrics_std(all_images, all_brain_recons, device):\n",
    "    all_images = all_images[:].to(device)\n",
    "    all_brain_recons = torch.stack([img for img in all_brain_recons[:]]).to(device).to(all_images.dtype).clamp(0,1).squeeze()\n",
    "\n",
    "    print(\"Images shape:\", all_images.shape)\n",
    "    print(\"Recons shape:\", all_brain_recons.shape)\n",
    "\n",
    "    # Ensure both tensors are the same size for MSE\n",
    "    resize = transforms.Resize((all_images.size(2), all_images.size(3)), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "    all_brain_recons = resize(all_brain_recons)\n",
    "\n",
    "    print(\"Images shape after resize:\", all_images.shape)\n",
    "    print(\"Recons shape after resize:\", all_brain_recons.shape)\n",
    "\n",
    "    # Preprocess\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    ])\n",
    "\n",
    "    # Flatten images while keeping the batch dimension\n",
    "    all_images_flattened = preprocess(all_images).reshape(len(all_images), -1).to(device).cpu()\n",
    "    all_brain_recons_flattened = preprocess(all_brain_recons).view(len(all_brain_recons), -1).cpu()\n",
    "\n",
    "    print(all_images_flattened.shape)\n",
    "    print(all_brain_recons_flattened.shape)\n",
    "\n",
    "    # PixCorr\n",
    "    print(\"\\n------calculating pixcorr------\")\n",
    "    corrsum = 0\n",
    "    for i in tqdm(range(len(all_images))):\n",
    "        corrsum += np.corrcoef(all_images_flattened[i], all_brain_recons_flattened[i])[0][1]\n",
    "    pixcorr_all = [np.corrcoef(all_images_flattened[i], all_brain_recons_flattened[i])[0][1] for i in range(len(all_images))]\n",
    "    pixcorr = np.mean(pixcorr_all)\n",
    "    pixcorr_std = np.std(pixcorr_all)\n",
    "    print(f\"PixCorr: {pixcorr:.4f} ± {pixcorr_std:.4f}\")\n",
    "\n",
    "    # SSIM\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(625, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    ])\n",
    "\n",
    "    img_gray = rgb2gray(preprocess(all_images).permute((0,2,3,1)).cpu().numpy())\n",
    "    recon_gray = rgb2gray(preprocess(all_brain_recons).permute((0,2,3,1)).cpu().numpy())\n",
    "    print(\"converted, now calculating ssim...\")\n",
    "\n",
    "    ssim_score=[]\n",
    "    for im, rec in tqdm(zip(img_gray, recon_gray), total=len(all_images)):\n",
    "        ssim_score.append(ssim(rec, im, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0))\n",
    "\n",
    "    ssim_mean = np.mean(ssim_score)\n",
    "    ssim_std = np.std(ssim_score)\n",
    "    print(f\"SSIM: {ssim_mean:.4f} ± {ssim_std:.4f}\")\n",
    "\n",
    "\n",
    "    # MSE\n",
    "    mse = torch.nn.functional.mse_loss(all_brain_recons, all_images).item()\n",
    "    print(\"MSE:\", mse)\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(all_brain_recons_flattened, all_images_flattened).mean().item()\n",
    "    cosine_sim_std = torch.nn.functional.cosine_similarity(all_brain_recons_flattened, all_images_flattened).std().item()\n",
    "    print(f\"Cosine Sim: {cosine_sim:.4f} ± {cosine_sim_std:.4f}\")\n",
    "\n",
    "\n",
    "    # Feature-based evaluations using different models\n",
    "    def evaluate_model(model, preprocess, feature_layers, layer_names):\n",
    "        results = {}\n",
    "        for feature_layer, layer_name in zip(feature_layers, layer_names):\n",
    "            print(f\"\\n---{layer_name}---\")\n",
    "            all_per_correct = two_way_identification(all_brain_recons.to(device).float(), all_images, \n",
    "                                                     model, preprocess, feature_layer, device=device)\n",
    "            results[layer_name] = {\n",
    "            'mean': np.mean(all_per_correct),\n",
    "            'std': np.std(all_per_correct)\n",
    "        }\n",
    "        print(f\"2-way Percent Correct: {results[layer_name]['mean']:.4f} ± {results[layer_name]['std']:.4f}\")\n",
    "        return results\n",
    "\n",
    "    # AlexNet\n",
    "    from torchvision.models import alexnet, AlexNet_Weights\n",
    "    alex_weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "    alex_model = create_feature_extractor(alexnet(weights=alex_weights), return_nodes=['features.4', 'features.11']).to(device)\n",
    "    alex_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    alexnet_results = evaluate_model(alex_model, preprocess, ['features.4', 'features.11'], ['AlexNet(2)', 'AlexNet(5)'])\n",
    "    del alex_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # InceptionV3\n",
    "    from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "    inception_weights = Inception_V3_Weights.DEFAULT\n",
    "    inception_model = create_feature_extractor(inception_v3(weights=inception_weights), return_nodes=['avgpool']).to(device)\n",
    "    inception_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(342, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    inception_results = evaluate_model(inception_model, preprocess, ['avgpool'], ['InceptionV3'])\n",
    "    del inception_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # CLIP\n",
    "    clip_model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "    # clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                            std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    ])\n",
    "\n",
    "    all_per_correct = two_way_identification(all_brain_recons, all_images,\n",
    "                                            clip_model.encode_image, preprocess, None) # final layer\n",
    "    clip_results = np.mean(all_per_correct)\n",
    "    clip_results_std = np.std(all_per_correct)\n",
    "    print(f\"CLIP: {clip_results:.4f} ± {clip_results_std:.4f}\")\n",
    "\n",
    "    # EfficientNet\n",
    "    from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
    "    eff_weights = EfficientNet_B1_Weights.DEFAULT\n",
    "    eff_model = create_feature_extractor(efficientnet_b1(weights=eff_weights), return_nodes=['avgpool']).to(device)\n",
    "    eff_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(255, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    gt = eff_model(preprocess(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt), -1).cpu().numpy()\n",
    "    fake = eff_model(preprocess(all_brain_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake), -1).cpu().numpy()\n",
    "    # effnet_distance = np.array([sp.distance.correlation(gt[i], fake[i]) for i in range(len(gt))]).mean()\n",
    "    effnet_all = [sp.distance.correlation(gt[i], fake[i]) for i in range(len(gt))]\n",
    "    effnet_distance = np.mean(effnet_all)\n",
    "    effnet_std = np.std(effnet_all)\n",
    "    print(f\"EffNet Distance: {effnet_distance:.4f} ± {effnet_std:.4f}\")\n",
    "\n",
    "    del eff_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # SwAV\n",
    "    swav_model = torch.hub.load('facebookresearch/swav:main', 'resnet50')\n",
    "    swav_model = create_feature_extractor(swav_model, return_nodes=['avgpool']).to(device)\n",
    "    swav_model.eval().requires_grad_(False)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    gt = swav_model(preprocess(all_images))['avgpool']\n",
    "    gt = gt.reshape(len(gt), -1).cpu().numpy()\n",
    "    fake = swav_model(preprocess(all_brain_recons))['avgpool']\n",
    "    fake = fake.reshape(len(fake), -1).cpu().numpy()\n",
    "    # swav_distance = np.array([correlation(gt[i], fake[i]) for i in range(len(gt))]).mean()\n",
    "    swav_all = [correlation(gt[i], fake[i]) for i in range(len(gt))]\n",
    "    swav_distance = np.mean(swav_all)\n",
    "    swav_std = np.std(swav_all)\n",
    "    print(f\"SwAV Distance: {swav_distance:.4f} ± {swav_std:.4f}\")\n",
    "\n",
    "    del swav_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Save the results\n",
    "    metrics = {\n",
    "        'PixCorr': (pixcorr, pixcorr_std),\n",
    "        'SSIM': (ssim_mean, ssim_std),\n",
    "        'MSE': (mse, 0.0),  \n",
    "        'Cosine Similarity': (cosine_sim, cosine_sim_std), \n",
    "        'AlexNet(2)': (alexnet_results[\"AlexNet(2)\"]['mean'], alexnet_results[\"AlexNet(2)\"]['std']),\n",
    "        'AlexNet(5)': (alexnet_results[\"AlexNet(5)\"]['mean'], alexnet_results[\"AlexNet(5)\"]['std']),\n",
    "        'InceptionV3': (inception_results[\"InceptionV3\"]['mean'], inception_results[\"InceptionV3\"]['std']),\n",
    "        'CLIP': (clip_results, clip_results_std),\n",
    "        'EffNet Distance': (effnet_distance, effnet_std),\n",
    "        'SwAV Distance': (swav_distance, swav_std)\n",
    "    }\n",
    "\n",
    "    return metrics \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_subject_wise_metrics(all_images, all_brain_recons, device):\n",
    "    subject_results = {}\n",
    "\n",
    "    print(\"\\nProcessing subject...\")\n",
    "    metrics = cal_metrics_std(all_images, all_brain_recons, device)\n",
    "    subject_results[\"subject\"] = metrics\n",
    "\n",
    "    print(\"\\nMetrics:\")\n",
    "    for k, (mean, std) in metrics.items():\n",
    "        print(f\"{k}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "\n",
    "calculate_subject_wise_metrics(true_images_tensor, recon_images_tensor, 'cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS for Linear/TimeAtt:\n",
    "# PixCorr: 0.1402 ± 0.1626\n",
    "# SSIM: 0.3638 ± 0.1994\n",
    "# MSE: 0.1095 ± 0.0000\n",
    "# Cosine Similarity: 0.8107 ± 0.1075\n",
    "# AlexNet(2): 0.8876 ± 0.1614\n",
    "# AlexNet(5): 0.9542 ± 0.0791\n",
    "# InceptionV3: 0.8679 ± 0.2254\n",
    "# CLIP: 0.8785 ± 0.2012\n",
    "# EffNet Distance: 0.7916 ± 0.1441\n",
    "# SwAV Distance: 0.4915 ± 0.1105\n",
    "\n",
    "# METRICS for MLP/TimeAtt:\n",
    "# PixCorr: 0.1514 ± 0.1671\n",
    "# SSIM: 0.3563 ± 0.2017\n",
    "# MSE: 0.1106 ± 0.0000\n",
    "# Cosine Similarity: 0.8146 ± 0.1229\n",
    "# AlexNet(2): 0.8812 ± 0.1770\n",
    "# AlexNet(5): 0.9432 ± 0.0954\n",
    "# InceptionV3: 0.8361 ± 0.2363\n",
    "# CLIP: 0.8416 ± 0.2289\n",
    "# EffNet Distance: 0.8223 ± 0.1391\n",
    "# SwAV Distance: 0.5155 ± 0.1138\n",
    "\n",
    "# METRICS for Linear/AvgTime:\n",
    "# PixCorr: 0.1563 ± 0.1630\n",
    "# SSIM: 0.3416 ± 0.2062\n",
    "# MSE: 0.1072 ± 0.0000\n",
    "# Cosine Similarity: 0.8007 ± 0.1003\n",
    "# AlexNet(2): 0.8762 ± 0.1678\n",
    "# AlexNet(5): 0.9502 ± 0.0752\n",
    "# InceptionV3: 0.8079 ± 0.2303\n",
    "# CLIP: 0.8265 ± 0.2454\n",
    "# EffNet Distance: 0.8270 ± 0.1367\n",
    "# SwAV Distance: 0.5281 ± 0.1165\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP1-Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(425, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "])\n",
    "\n",
    "true_images_tensor_VIT = preprocess(true_images_tensor)\n",
    "recon_images_tensor_VIT = preprocess(recon_images_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_h_14_swag-80465313.pth\" to /home/matteoc/.cache/torch/hub/checkpoints/vit_h_14_swag-80465313.pth\n",
      "100%|██████████| 2.36G/2.36G [00:06<00:00, 381MB/s]\n",
      "Processing images: 100%|██████████| 100/100 [00:51<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean acc: 0.3239, Overall mean std: 0.2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics.functional import accuracy\n",
    "from torchvision.models import ViT_H_14_Weights, vit_h_14\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load ViT model and preprocessing\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = ViT_H_14_Weights.DEFAULT\n",
    "vit_model = vit_h_14(weights=weights).to(device)\n",
    "vit_model.eval()\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Function to compute accuracy\n",
    "def n_way_top_k_acc(pred, class_id, n_way, num_trials=40, top_k=1):\n",
    "    pick_range = [i for i in range(len(pred)) if i != class_id]\n",
    "    acc_list = []\n",
    "    for _ in range(num_trials):\n",
    "        idxs_picked = np.random.choice(pick_range, n_way - 1, replace=False)\n",
    "        pred_picked = torch.cat([pred[class_id].unsqueeze(0), pred[idxs_picked]])\n",
    "        acc = accuracy(pred_picked.unsqueeze(0), torch.tensor([0], device=pred.device), \n",
    "                       task='multiclass', num_classes=n_way, top_k=top_k)\n",
    "        acc_list.append(acc.item())\n",
    "    return np.mean(acc_list), np.std(acc_list)\n",
    "\n",
    "# Initialize storage\n",
    "all_acc = []\n",
    "all_std = []\n",
    "\n",
    "# Process images\n",
    "for i in tqdm(range(len(recon_images_tensor_VIT)), desc=\"Processing images\"):\n",
    "    # Preprocess images\n",
    "    image = preprocess(true_images_tensor_VIT[i].unsqueeze(0)).to(device)\n",
    "    recon_image = preprocess(recon_images_tensor_VIT[i].unsqueeze(0)).to(device)\n",
    "\n",
    "    # Get model outputs\n",
    "    recon_image_out = vit_model(recon_image).squeeze(0).softmax(0).detach()\n",
    "    gt_class_id = vit_model(image).squeeze(0).softmax(0).argmax().item()\n",
    "\n",
    "    # Compute accuracy\n",
    "    acc, std = n_way_top_k_acc(recon_image_out, gt_class_id, 50, 1000, 1)\n",
    "    all_acc.append(acc)\n",
    "    all_std.append(std)\n",
    "\n",
    "# Compute and print final results\n",
    "mean_acc = np.mean(all_acc)\n",
    "mean_std = np.mean(all_std)\n",
    "\n",
    "print(f\"Overall mean acc: {mean_acc:.4f}, Overall mean std: {mean_std:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-meg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
